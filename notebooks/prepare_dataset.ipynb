{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "moJ46xEYJFt8",
        "av3tMtIPtuyU",
        "l7zKQ52ExX_d"
      ],
      "history_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install Conda\n",
        "The runtime will restart after installation, please execute the remaining cells after the restart."
      ],
      "metadata": {
        "id": "moJ46xEYJFt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8vSGcVLx7zt",
        "outputId": "808f6826-fa95-43ee-f3f3-6f03d1b421ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m✨🍰✨ Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup dependencies"
      ],
      "metadata": {
        "id": "Yat81N2Bt5KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYFbIeW2sv30",
        "outputId": "bfba434a-d997-431c-bcb4-2e1e5847143b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUca-LTTc4fi",
        "outputId": "21f7b84b-bb0a-4cc2-a540-96e32555bcf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'FastSpeech2' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/ming024/FastSpeech2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml==6.0 unidecode==1.3.6 tgt==1.4.4 pyworld==0.2.10\n",
        "!pip install librosa==0.9.2 numba==0.57.0\n",
        "!pip install --force-reinstall numpy==1.24.3\n",
        "\n",
        "#if you run into issues, try installing with --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qwtygw1aEGv",
        "outputId": "69a7e967-da7f-499c-e07b-c65bf1fc548e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml==6.0 in /usr/local/lib/python3.10/site-packages (6.0)\n",
            "Requirement already satisfied: unidecode==1.3.6 in /usr/local/lib/python3.10/site-packages (1.3.6)\n",
            "Requirement already satisfied: tgt==1.4.4 in /usr/local/lib/python3.10/site-packages (1.4.4)\n",
            "Requirement already satisfied: pyworld==0.2.10 in /usr/local/lib/python3.10/site-packages (0.2.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from pyworld==0.2.10) (1.24.3)\n",
            "Requirement already satisfied: cython>=0.24.0 in /usr/local/lib/python3.10/site-packages (from pyworld==0.2.10) (0.29.34)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa==0.9.2 in /usr/local/lib/python3.10/site-packages (0.9.2)\n",
            "Requirement already satisfied: numba==0.57.0 in /usr/local/lib/python3.10/site-packages (0.57.0)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.10/site-packages (from librosa==0.9.2) (0.12.1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.10/site-packages (from librosa==0.9.2) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/site-packages (from librosa==0.9.2) (1.24.3)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/site-packages (from librosa==0.9.2) (3.0.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/site-packages (from librosa==0.9.2) (1.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from librosa==0.9.2) (23.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/site-packages (from librosa==0.9.2) (1.2.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/site-packages (from librosa==0.9.2) (1.7.0)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/site-packages (from librosa==0.9.2) (5.1.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/site-packages (from librosa==0.9.2) (1.10.1)\n",
            "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.10/site-packages (from numba==0.57.0) (0.40.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.2) (3.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.2) (2.28.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=0.19.1->librosa==0.9.2) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/site-packages (from soundfile>=0.10.2->librosa==0.9.2) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2) (2.21)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (3.1.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.24.3\n",
            "  Using cached numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "Successfully installed numpy-1.24.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "abIprDRxs3ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install MFA"
      ],
      "metadata": {
        "id": "pieRp9Uvx6az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -c conda-forge montreal-forced-aligner"
      ],
      "metadata": {
        "id": "KO5ZAgRdyIDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create MFA user account"
      ],
      "metadata": {
        "id": "OQbVwGsJL7cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MFA commands must be run as unprivileged user\n",
        "!useradd -m -d /home/mfa_user mfa_user\n",
        "!su - mfa_user -c \"echo hello as mfa_user\"\n",
        "\n",
        "%mkdir /home/mfa_user\n",
        "!chown -hR mfa_user /home/mfa_user\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "towszJaAL61o",
        "outputId": "a094d347-a19e-473d-f039-3425792e28bb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "useradd: user 'mfa_user' already exists\n",
            "hello as mfa_user\n",
            "mkdir: cannot create directory ‘/home/mfa_user’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dependencies"
      ],
      "metadata": {
        "id": "t1mpUQOWMkXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Excellent MFA tutorial: https://eleanorchodroff.com/mfa_tutorial.html\n",
        "acoustic_model = 'english_us_arpa' #@param\n",
        "dictionary_model = 'english_us_arpa' #@param\n",
        "\n",
        "!su - mfa_user -c \"mfa version\"\n",
        "!su - mfa_user -c \"mfa model download acoustic $acoustic_model\"\n",
        "!su - mfa_user -c \"mfa model download dictionary $dictionary_model\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3juI4ATMzXVI",
        "outputId": "f0ec3e9a-7e7a-4e57-fa75-3ece7f38cbfd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.10\n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Saved model to                                                        \n",
            "\u001b[2;36m \u001b[0m         \u001b[35m/home/mfa_user/Documents/MFA/pretrained_models/acoustic/\u001b[0m\u001b[95menglish_us_arp\u001b[0m\n",
            "\u001b[2;36m \u001b[0m         \u001b[95ma.zip\u001b[0m, you can now use english_us_arpa in place of acoustic paths in  \n",
            "\u001b[2;36m \u001b[0m         mfa commands.                                                         \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Saved model to                                                        \n",
            "\u001b[2;36m \u001b[0m         \u001b[35m/home/mfa_user/Documents/MFA/pretrained_models/dictionary/\u001b[0m\u001b[95menglish_us_a\u001b[0m\n",
            "\u001b[2;36m \u001b[0m         \u001b[95mrpa.dict\u001b[0m, you can now use english_us_arpa in place of dictionary paths\n",
            "\u001b[2;36m \u001b[0m         in mfa commands.                                                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset requirements and settings\n",
        "\n",
        "* dataset_path: A directory with the raw audio files + text transcriptions. The text and audio file names should match."
      ],
      "metadata": {
        "id": "av3tMtIPtuyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# The input dataset\n",
        "dataset_name = 'MyDataset' #@param {type:'string'}\n",
        "dataset_path = '/gdrive/MyDrive/subset' #@param {type:'string'}\n",
        "speaker_name = 'universal' #@param {type:'string'}\n",
        "\n",
        "# The output folder for processed data\n",
        "output_path = '/content/output_dataset' #@param {type:'string'}\n",
        "\n",
        "# MFA settings\n",
        "text_file_extension = '.lab' #@param ['.txt','.lab']\n",
        "corpus_name = 'metadata.csv' #@param {type:'string'}\n",
        "lexicon_path = '/content/FastSpeech2/lexicon/librispeech-lexicon.txt' #@param {type:'string'}\n",
        "allow_overwrite_existing_corpus = True #@param {type:'boolean'}\n",
        "\n",
        "acoustic_model = 'english_us_arpa' #@param {type:'string'}\n",
        "dictionary_file = '/content/FastSpeech2/lexicon/librispeech-lexicon.txt' #@param {type:'string'}\n",
        "\n",
        "# Paths\n",
        "preprocessed_data_path = os.path.join(output_path, 'preprocessed_data')\n",
        "preprocessed_data_speaker_path = os.path.join(output_path, 'preprocessed_data',\n",
        "                                              dataset_name)\n",
        "raw_data_path = os.path.join(output_path, 'raw_data')\n",
        "raw_data_speaker_path = os.path.join(output_path, 'raw_data', speaker_name)\n",
        "corpus_path = raw_data_speaker_path\n",
        "corpus_file_path = os.path.join(corpus_path, corpus_name)\n",
        "mfa_output_path = os.path.join('/home/mfa_user', dataset_name, 'TextGrid')\n",
        "textgrid_dir = os.path.join(preprocessed_data_speaker_path, 'TextGrid', speaker_name)\n",
        "\n",
        "# Create directory structure\n",
        "%mkdir -p $output_path\n",
        "%mkdir -p $corpus_path\n",
        "%mkdir -p $preprocessed_data_speaker_path\n",
        "%mkdir -p $raw_data_speaker_path\n",
        "%mkdir -p $textgrid_dir"
      ],
      "metadata": {
        "id": "yrU71RMluboT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup MFA user account"
      ],
      "metadata": {
        "id": "RNuVZzqXLxTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug utilities\n",
        "Copy files"
      ],
      "metadata": {
        "id": "l7zKQ52ExX_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src = '/gdrive/MyDrive/dataset/raw_data/universal' #@param {type:'string'}\n",
        "dest = '/gdrive/MyDrive/subset/'\n",
        "\n",
        "%mkdir $dest\n",
        "\n",
        "for i in range(0,9):\n",
        "  fname = f'p303_00{i+1}'\n",
        "  wav = os.path.join(src, fname + '.wav')\n",
        "  lab = os.path.join(src, fname + '.lab')\n",
        "  dest_path = os.path.join(dest)\n",
        "  shutil.copy(wav, dest_path)\n",
        "  shutil.copy(lab, dest_path)\n",
        "%ls $dest\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsIct9Izt_i_",
        "outputId": "aa595054-8e9f-4072-cc29-e8bd1acc1a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/gdrive/MyDrive/subset/’: File exists\n",
            "p303_001.lab  p303_003.lab  p303_005.lab  p303_007.lab  p303_009.lab\n",
            "p303_001.wav  p303_003.wav  p303_005.wav  p303_007.wav  p303_009.wav\n",
            "p303_002.lab  p303_004.lab  p303_006.lab  p303_008.lab\n",
            "p303_002.wav  p303_004.wav  p303_006.wav  p303_008.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Data"
      ],
      "metadata": {
        "id": "G_gn_upsCfgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make metadata.csv corpus\n",
        "Saved to output_path/raw_data/speaker_name"
      ],
      "metadata": {
        "id": "WwgRs06k1Z0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Don't overwrite existing file\n",
        "if not allow_overwrite_existing_corpus:\n",
        "  assert(not os.path.exists(corpus_file_path)), 'Corpus file already exists, enable `allow_overwrite_existing_corpus` to disable this behavior.'\n",
        "\n",
        "\n",
        "def concatenate_file_contents(filename):\n",
        "    \"\"\" Reads text file and outputs string with name of file and contents \"\"\"\n",
        "    filename_no_ext = str(os.path.basename(filename)).replace(text_file_extension,'')\n",
        "    with open(filename, 'r') as file:\n",
        "      contents = file.read().strip()\n",
        "      result = f\"{filename_no_ext}|{contents}|{contents}\\r\\n\"\n",
        "      return result\n",
        "\n",
        "\n",
        "def process_files_in_path(text_files_path, output_corpus_file_path):\n",
        "    \"\"\" Open a file at output_corpus_path and write formatted data to it \"\"\"\n",
        "    with open(output_corpus_file_path, 'w') as f:\n",
        "        # Get all .txt files in the specified path\n",
        "        txt_files = [file for file in os.listdir(text_files_path) if file.endswith(text_file_extension)]\n",
        "        txt_files_count = len(txt_files)\n",
        "        if txt_files_count <= 0:\n",
        "          print(f'No text files with extension {text_file_extension} found in {text_files_path}, try changing `text_file_extension` in settings')\n",
        "        # Process each file and concatenate the contents\n",
        "        for file in txt_files:\n",
        "          file_path = os.path.join(text_files_path, file)\n",
        "          output = concatenate_file_contents(file_path)\n",
        "          f.write(output)\n",
        "# Run\n",
        "print(f'Dataset path: {dataset_path}')\n",
        "print(f'Corpus path: {corpus_file_path}')\n",
        "process_files_in_path(dataset_path, corpus_file_path)\n",
        "print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT9s7mZH1OMa",
        "outputId": "66fbb296-e6e6-4e24-a87b-6269e93cabbd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset path: /gdrive/MyDrive/subset\n",
            "Corpus path: /content/output_dataset/raw_data/universal/metadata.csv\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make TextGrids"
      ],
      "metadata": {
        "id": "f_lDZDAzWYDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new configuration files"
      ],
      "metadata": {
        "id": "dG88YPpkWe2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "try:\n",
        "    from yaml import CLoader as Loader, CDumper as Dumper\n",
        "except ImportError:\n",
        "    from yaml import Loader, Dumper\n",
        "\n",
        "config_dir = f'/content/FastSpeech2/config/{dataset_name}'\n",
        "copied_config_dir = os.path.join(output_path, 'configs')\n",
        "\n",
        "!mkdir -p $config_dir \n",
        "!mkdir $copied_config_dir\n",
        "!cp -r /content/FastSpeech2/config/LJSpeech/* $config_dir\n",
        "\n",
        "\n",
        "def get_yaml_path(name):\n",
        "  return os.path.join(config_dir, name+'.yaml')\n",
        "\n",
        "\n",
        "def get_yaml_contents(name):\n",
        "  with open(get_yaml_path(name), 'r') as f:\n",
        "    return yaml.safe_load(f.read())\n",
        "  \n",
        "\n",
        "def write_yaml(name, contents):\n",
        "  with open(get_yaml_path(name), 'w') as f:\n",
        "      f.write(yaml.dump(contents))\n",
        "\n",
        "\n",
        "# model.yaml - change speaker name\n",
        "model = get_yaml_contents('model')\n",
        "model['vocoder']['speaker'] = speaker_name\n",
        "write_yaml('model', model)\n",
        "\n",
        "# preprocess.yaml - update paths and add field to text\n",
        "pp = get_yaml_contents('preprocess')\n",
        "pp['dataset'] = dataset_name\n",
        "pp['path']['corpus_path'] = corpus_path\n",
        "pp['path']['lexicon_path'] = lexicon_path\n",
        "pp['path']['raw_path'] = raw_data_path\n",
        "pp['path']['preprocessed_path'] = preprocessed_data_speaker_path\n",
        "pp['preprocessing']['text']['max_length'] = 4096  # Needed for training EfficientSpeech models\n",
        "write_yaml('preprocess', pp)\n",
        "\n",
        "# train.yaml - update paths\n",
        "tr = get_yaml_contents('train')\n",
        "tr['path']['ckpt_path'] = f'./output/ckpt/{dataset_name}'\n",
        "tr['path']['log_path'] = f'./output/log/{dataset_name}'\n",
        "tr['path']['result_path'] = f'./output/result/{dataset_name}'\n",
        "write_yaml('train', tr)\n",
        "\n",
        "print(f'Wrote configs in {config_dir}, copying to {copied_config_dir}')\n",
        "!cp -r $config_dir $copied_config_dir\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "aMx1ij0HWbLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513c8ab2-6e2e-43cf-b59d-01b9724a47f1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/output_dataset/configs’: File exists\n",
            "Wrote configs in /content/FastSpeech2/config/MyDataset, copying to /content/output_dataset/configs\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare align\n",
        "The following code is modified from https://github.com/ming024/FastSpeech2/blob/master/preprocessor/ljspeech.py\n"
      ],
      "metadata": {
        "id": "xIxc6NUtfuKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "try:\n",
        "    from yaml import CLoader as Loader, CDumper as Dumper\n",
        "except ImportError:\n",
        "    from yaml import Loader, Dumper\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Workaround for importing text\n",
        "import sys\n",
        "sys.path.append('/content/FastSpeech2')\n",
        "from text import _clean_text\n",
        "\n",
        "\n",
        "def prepare_align(config):\n",
        "    sampling_rate = config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n",
        "    max_wav_value = config[\"preprocessing\"][\"audio\"][\"max_wav_value\"]\n",
        "    cleaners = config[\"preprocessing\"][\"text\"][\"text_cleaners\"]\n",
        "    speaker = speaker_name\n",
        "    with open(corpus_file_path, encoding=\"utf-8\") as f:\n",
        "        for line in tqdm(f):\n",
        "            parts = line.strip().split(\"|\")\n",
        "            base_name = parts[0]\n",
        "            text = parts[2]\n",
        "            text = _clean_text(text, cleaners)\n",
        "\n",
        "            wav_path = os.path.join(dataset_path, \"{}.wav\".format(base_name))\n",
        "            if os.path.exists(wav_path):\n",
        "                os.makedirs(raw_data_speaker_path, exist_ok=True)\n",
        "                wav, sr = librosa.load(wav_path, sr=sampling_rate)\n",
        "                #wav, _ = librosa.load(wav_path, sampling_rate)\n",
        "                wav = wav / max(abs(wav)) * max_wav_value\n",
        "                wavfile.write(\n",
        "                    os.path.join(raw_data_speaker_path, \"{}.wav\".format(base_name)),\n",
        "                    sampling_rate,\n",
        "                    wav.astype(np.int16),\n",
        "                )\n",
        "                with open(\n",
        "                    os.path.join(raw_data_speaker_path, \"{}.lab\".format(base_name)),\n",
        "                    \"w\",\n",
        "                ) as f1:\n",
        "                    f1.write(text)\n",
        "\n",
        "\n",
        "config = get_yaml_contents('preprocess')\n",
        "prepare_align(config)\n",
        "print('Prepare align done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PXl_dbkf6xk",
        "outputId": "94817d5c-b1ae-4c10-8347-3f0adfe92622"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9it [00:14,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare align done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Forced alignment\n"
      ],
      "metadata": {
        "id": "kHqaO0dcCofH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1RxsTDzkEC__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output TextGrid files go here\n",
        "!su - mfa_user -c \"mkdir -p $mfa_output_path\"\n",
        "%mkdir -p $textgrid_dir\n",
        "\n",
        "# Allow mfa_user access to output directory \n",
        "!chown mfa_user $textgrid_dir\n",
        "\n",
        "# Command line options\n",
        "# -m fast: immediate disconnect (doesn't work sadface)\n",
        "# --clean: cleans output dir for subsequent runs (if off, \n",
        "#             does not overwrite old data)\n",
        "# --single_speaker: multiprocessing for only one speaker\n",
        "mfa_cmd_opts = f'--clean --single_speaker'\n",
        "align_cmd_opts = f'{corpus_path} {dictionary_file} {acoustic_model} {mfa_output_path}'\n",
        "\n",
        "# Command must be run as unprivileged user\n",
        "!echo Running mfa align with arguments: $mfa_cmd_opts $align_cmd_opts\n",
        "!su - mfa_user -c \"mfa align $mfa_cmd_opts $align_cmd_opts\"\n",
        "\n",
        "!echo Copying TextGrid files to $textgrid_dir\n",
        "!cp $mfa_output_path/*.* $textgrid_dir "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrR33GuhDArV",
        "outputId": "e510095f-b2f6-4e4a-bab1-27e33b04e67a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running mfa align with arguments: --clean --single_speaker /content/output_dataset/raw_data/universal /content/FastSpeech2/lexicon/librispeech-lexicon.txt english_us_arpa /home/mfa_user/MyDataset/TextGrid\n",
            "The global MFA database server does not exist, initializing it first.\n",
            "waiting for server to start.... done\n",
            "server started\n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n",
            "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m9\u001b[0m files, average number of utterances per      \n",
            "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m9.0\u001b[0m                                                          \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n",
            "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9 \u001b[0m [ \u001b[33m0:00:18\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m1 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split for feature generation\u001b[33m...\u001b[0m                       \n",
            "\u001b[2K\u001b[35m  39%\u001b[0m \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/18 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n",
            "\u001b[2K\u001b[35m  67%\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m6/9 \u001b[0m [ \u001b[33m0:00:20\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m36 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n",
            "\u001b[2K\u001b[35m  78%\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m7/9 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split with features\u001b[33m...\u001b[0m                                \n",
            "\u001b[2K\u001b[35m  11%\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/9 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
            "\u001b[2K\u001b[35m  33%\u001b[0m \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/9 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n",
            "\u001b[2K\u001b[35m  78%\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m7/9 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m42 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating fMLLR for speaker adaptation\u001b[33m...\u001b[0m                           \n",
            "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing second-pass alignment\u001b[33m...\u001b[0m                                   \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n",
            "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m20 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n",
            "\u001b[2K\u001b[35m  44%\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/9 \u001b[0m [ \u001b[33m0:00:11\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m22 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n",
            "\u001b[2K\u001b[35m  44%\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/9 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to \u001b[35m/home/mfa_user/MyDataset/\u001b[0m\u001b[95mTextGrid...\u001b[0m \n",
            "\u001b[2K\u001b[35m  11%\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/9 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to \u001b[35m/home/mfa_user/MyDataset/\u001b[0m\u001b[95mTextGrid\u001b[0m!    \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m136.649\u001b[0m seconds                                 \n",
            "pg_ctl stdout: waiting for server to shut down............................................................... failed\n",
            "\n",
            "pg_ctl stderr: pg_ctl: server does not shut down\n",
            "HINT: The \"-m fast\" option immediately disconnects sessions rather than\n",
            "waiting for session-initiated disconnection.\n",
            "\n",
            "Exception ignored in atexit callback: <function stop_server at 0x7f389bc73f40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/site-packages/montreal_forced_aligner/command_line/utils.py\", line 470, in stop_server\n",
            "    raise DatabaseError(\n",
            "montreal_forced_aligner.exceptions.DatabaseError: DatabaseError:\n",
            "\n",
            "There was an error encountered starting the global MFA database server, please see /home/mfa_user/Documents/MFA/pg_log_global.txt for more details and/or look at the logged errors above.\n",
            "Copying TextGrid files to /content/output_dataset/preprocessed_data/MyDataset/TextGrid/universal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess textgrids"
      ],
      "metadata": {
        "id": "sWT4zYzl47Vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you run into errors, install older numpy\n",
        "\n",
        "!pip install --force-reinstall numpy==1.20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKMcs7kMnSIN",
        "outputId": "a63210e4-737a-44dc-8601-a10c8c862e7c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.20\n",
            "  Downloading numpy-1.20.0.zip (8.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: numpy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for numpy \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for numpy\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build numpy\n",
            "\u001b[31mERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/FastSpeech2')\n",
        "from preprocessor.preprocessor import Preprocessor\n",
        "\n",
        "config = get_yaml_contents('preprocess')\n",
        "preprocessor = Preprocessor(config)\n",
        "preprocessor.build_from_path()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "id": "eEjO6Np05TFh",
        "outputId": "5a68ee53-df07-4e4d-e3ea-3e0a73ca5d5f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.19.0\n",
            "  Using cached numpy-1.19.0.zip (7.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m__init__.pxd\u001b[0m in \u001b[0;36mnumpy.import_array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-2650e7971280>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/FastSpeech2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_yaml_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preprocess'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/FastSpeech2/preprocessor/preprocessor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyworld\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterp1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/site-packages/pyworld/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pyworld'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpyworld\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32mpyworld/pyworld.pyx\u001b[0m in \u001b[0;36minit pyworld.pyworld\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m__init__.pxd\u001b[0m in \u001b[0;36mnumpy.import_array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}