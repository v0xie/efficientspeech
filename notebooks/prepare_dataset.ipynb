{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l7zKQ52ExX_d"
      ],
      "history_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Conda\n",
        "The runtime will restart after installation, please execute the remaining cells after the restart."
      ],
      "metadata": {
        "id": "moJ46xEYJFt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8vSGcVLx7zt",
        "outputId": "808f6826-fa95-43ee-f3f3-6f03d1b421ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m✨🍰✨ Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup dependencies"
      ],
      "metadata": {
        "id": "Yat81N2Bt5KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYFbIeW2sv30",
        "outputId": "0327f43b-cd4a-4418-f866-403dc8dd9344"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUca-LTTc4fi",
        "outputId": "ec77606b-a9fb-4299-ad39-8ebd1207e254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'FastSpeech2'...\n",
            "remote: Enumerating objects: 991, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 991 (delta 10), reused 7 (delta 7), pack-reused 978\u001b[K\n",
            "Receiving objects: 100% (991/991), 330.31 MiB | 18.59 MiB/s, done.\n",
            "Resolving deltas: 100% (175/175), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/ming024/FastSpeech2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml==6.0 unidecode==1.3.6 tgt==1.4.4 pyworld==0.2.10\n",
        "!pip install librosa==0.9.2 numpy==1.24.3 numba==0.57.0\n",
        "\n",
        "#if you run into issues, try installing with --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-qwtygw1aEGv",
        "outputId": "0f08c301-b05b-4cc1-8636-bf0f1d1f8ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (6.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/site-packages (1.3.6)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting librosa\n",
            "  Using cached librosa-0.10.0.post2-py3-none-any.whl (253 kB)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Collecting numba\n",
            "  Downloading numba-0.57.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.1.1\n",
            "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting msgpack>=1.0\n",
            "  Downloading msgpack-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.8/316.8 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib>=0.14\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting decorator>=4.3.0\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting scipy>=1.2.0\n",
            "  Using cached scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "Collecting soundfile>=0.12.1\n",
            "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pooch<1.7,>=1.0\n",
            "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting audioread>=2.1.9\n",
            "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn>=0.20.0\n",
            "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lazy-loader>=0.1\n",
            "  Downloading lazy_loader-0.2-py3-none-any.whl (8.6 kB)\n",
            "Collecting soxr>=0.3.2\n",
            "  Downloading soxr-0.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llvmlite<0.41,>=0.40.0dev0\n",
            "  Downloading llvmlite-0.40.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.19.0\n",
            "  Downloading requests-2.30.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting appdirs>=1.3.0\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting cffi>=1.0\n",
            "  Downloading cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.8/441.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycparser\n",
            "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.2/123.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.0/157.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: audioread\n",
            "  Building wheel for audioread (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23706 sha256=3d7ea7af34d104a1b1563663dce1d789f36d289c08b155390b695205869096d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/4b/39/c5f6c4ee93b43281dda4dab5ac5f2bdf9d11074d427493cd55\n",
            "Successfully built audioread\n",
            "Installing collected packages: msgpack, appdirs, urllib3, typing-extensions, threadpoolctl, pycparser, packaging, numpy, llvmlite, lazy-loader, joblib, idna, decorator, charset-normalizer, certifi, audioread, soxr, scipy, requests, numba, cffi, soundfile, scikit-learn, pooch, librosa\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.0.5\n",
            "    Uninstalling msgpack-1.0.5:\n",
            "      Successfully uninstalled msgpack-1.0.5\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.15\n",
            "    Uninstalling urllib3-1.26.15:\n",
            "      Successfully uninstalled urllib3-1.26.15\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.1.0\n",
            "    Uninstalling threadpoolctl-3.1.0:\n",
            "      Successfully uninstalled threadpoolctl-3.1.0\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.21\n",
            "    Uninstalling pycparser-2.21:\n",
            "      Successfully uninstalled pycparser-2.21\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.1\n",
            "    Uninstalling packaging-23.1:\n",
            "      Successfully uninstalled packaging-23.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.39.1\n",
            "    Uninstalling llvmlite-0.39.1:\n",
            "      Successfully uninstalled llvmlite-0.39.1\n",
            "  Attempting uninstall: lazy-loader\n",
            "    Found existing installation: lazy_loader 0.2\n",
            "    Uninstalling lazy_loader-0.2:\n",
            "      Successfully uninstalled lazy_loader-0.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.2.0\n",
            "    Uninstalling joblib-1.2.0:\n",
            "      Successfully uninstalled joblib-1.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 5.1.1\n",
            "    Uninstalling decorator-5.1.1:\n",
            "      Successfully uninstalled decorator-5.1.1\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.1.0\n",
            "    Uninstalling charset-normalizer-3.1.0:\n",
            "      Successfully uninstalled charset-normalizer-3.1.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.5.7\n",
            "    Uninstalling certifi-2023.5.7:\n",
            "      Successfully uninstalled certifi-2023.5.7\n",
            "  Attempting uninstall: audioread\n",
            "    Found existing installation: audioread 3.0.0\n",
            "    Uninstalling audioread-3.0.0:\n",
            "      Successfully uninstalled audioread-3.0.0\n",
            "  Attempting uninstall: soxr\n",
            "    Found existing installation: soxr 0.3.5\n",
            "    Uninstalling soxr-0.3.5:\n",
            "      Successfully uninstalled soxr-0.3.5\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.28.2\n",
            "    Uninstalling requests-2.28.2:\n",
            "      Successfully uninstalled requests-2.28.2\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.56.4\n",
            "    Uninstalling numba-0.56.4:\n",
            "      Successfully uninstalled numba-0.56.4\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.15.1\n",
            "    Uninstalling cffi-1.15.1:\n",
            "      Successfully uninstalled cffi-1.15.1\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: soundfile 0.12.1\n",
            "    Uninstalling soundfile-0.12.1:\n",
            "      Successfully uninstalled soundfile-0.12.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: pooch\n",
            "    Found existing installation: pooch 1.7.0\n",
            "    Uninstalling pooch-1.7.0:\n",
            "      Successfully uninstalled pooch-1.7.0\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.0\n",
            "    Uninstalling librosa-0.10.0:\n",
            "      Successfully uninstalled librosa-0.10.0\n",
            "Successfully installed appdirs-1.4.4 audioread-3.0.0 certifi-2023.5.7 cffi-1.15.1 charset-normalizer-3.1.0 decorator-5.1.1 idna-3.4 joblib-1.2.0 lazy-loader-0.2 librosa-0.10.0.post2 llvmlite-0.40.0 msgpack-1.0.5 numba-0.57.0 numpy-1.24.3 packaging-23.1 pooch-1.6.0 pycparser-2.21 requests-2.30.0 scikit-learn-1.2.2 scipy-1.10.1 soundfile-0.12.1 soxr-0.3.5 threadpoolctl-3.1.0 typing-extensions-4.5.0 urllib3-2.0.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_cffi_backend",
                  "_soundfile",
                  "_soundfile_data",
                  "audioread",
                  "cffi",
                  "librosa",
                  "numpy",
                  "soundfile",
                  "soxr"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "abIprDRxs3ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset requirements and settings\n",
        "Each file has an accompanying text file with the transcription"
      ],
      "metadata": {
        "id": "av3tMtIPtuyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# The input dataset\n",
        "dataset_name = 'MyDataset' #@param {type:'string'}\n",
        "dataset_path = '/gdrive/MyDrive/subset' #@param {type:'string'}\n",
        "speaker_name = 'universal' #@param {type:'string'}\n",
        "\n",
        "# The output folder for processed data\n",
        "output_path = '/content/output_dataset' #@param {type:'string'}\n",
        "\n",
        "# MFA settings\n",
        "text_file_extension = '.lab' #@param ['.txt','.lab']\n",
        "corpus_name = 'metadata.csv' #@param {type:'string'}\n",
        "lexicon_path = '/content/FastSpeech2/lexicon/librispeech-lexicon.txt' #@param {type:'string'}\n",
        "allow_overwrite_existing_corpus = True #@param {type:'boolean'}\n",
        "\n",
        "acoustic_model = 'english_us_arpa' #@param {type:'string'}\n",
        "dictionary_file = '/content/FastSpeech2/lexicon/librispeech-lexicon.txt' #@param {type:'string'}\n",
        "\n",
        "# Paths\n",
        "preprocessed_data_path = os.path.join(output_path, 'preprocessed_data')\n",
        "preprocessed_data_speaker_path = os.path.join(output_path, 'preprocessed_data',\n",
        "                                              dataset_name)\n",
        "raw_data_path = os.path.join(output_path, 'raw_data')\n",
        "raw_data_speaker_path = os.path.join(output_path, 'raw_data', speaker_name)\n",
        "corpus_path = raw_data_speaker_path\n",
        "corpus_file_path = os.path.join(corpus_path, corpus_name)\n",
        "mfa_output_path = os.path.join('/home/mfa_user', dataset_name, 'TextGrid')\n",
        "textgrid_dir = os.path.join(preprocessed_data_speaker_path, 'TextGrid', speaker_name)\n",
        "\n",
        "# Create directory structure\n",
        "%mkdir -p $output_path\n",
        "%mkdir -p $corpus_path\n",
        "%mkdir -p $preprocessed_data_speaker_path\n",
        "%mkdir -p $raw_data_speaker_path\n",
        "%mkdir -p $textgrid_dir"
      ],
      "metadata": {
        "id": "yrU71RMluboT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install MFA"
      ],
      "metadata": {
        "id": "pieRp9Uvx6az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -c conda-forge montreal-forced-aligner"
      ],
      "metadata": {
        "id": "KO5ZAgRdyIDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d81266-6e2c-452a-8685-a62d91fa1927"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bfailed\n",
            "\n",
            "CondaError: KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Excellent MFA tutorial: https://eleanorchodroff.com/mfa_tutorial.html\n",
        "acoustic_model = 'english_us_arpa' #@param\n",
        "dictionary_model = 'english_us_arpa' #@param\n",
        "\n",
        "# Command must be run as unprivileged user\n",
        "!useradd mfa_user\n",
        "!su - mfa_user -c \"echo hello as mfa_user\"\n",
        "\n",
        "!su - mfa_user -c \"mfa version\"\n",
        "!su - mfa_user -c \"mfa model download acoustic $acoustic_model\"\n",
        "!su - mfa_user -c \"mfa model download dictionary $dictionary_model\""
      ],
      "metadata": {
        "id": "3juI4ATMzXVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug utilities\n",
        "Copy files"
      ],
      "metadata": {
        "id": "l7zKQ52ExX_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src = '/gdrive/MyDrive/dataset/raw_data/universal' #@param {type:'string'}\n",
        "dest = '/gdrive/MyDrive/subset/'\n",
        "\n",
        "%mkdir $dest\n",
        "\n",
        "for i in range(0,9):\n",
        "  fname = f'p303_00{i+1}'\n",
        "  wav = os.path.join(src, fname + '.wav')\n",
        "  lab = os.path.join(src, fname + '.lab')\n",
        "  dest_path = os.path.join(dest)\n",
        "  shutil.copy(wav, dest_path)\n",
        "  shutil.copy(lab, dest_path)\n",
        "%ls $dest\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsIct9Izt_i_",
        "outputId": "aa595054-8e9f-4072-cc29-e8bd1acc1a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/gdrive/MyDrive/subset/’: File exists\n",
            "p303_001.lab  p303_003.lab  p303_005.lab  p303_007.lab  p303_009.lab\n",
            "p303_001.wav  p303_003.wav  p303_005.wav  p303_007.wav  p303_009.wav\n",
            "p303_002.lab  p303_004.lab  p303_006.lab  p303_008.lab\n",
            "p303_002.wav  p303_004.wav  p303_006.wav  p303_008.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Data"
      ],
      "metadata": {
        "id": "G_gn_upsCfgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make metadata.csv corpus\n",
        "Saved to output_path/raw_data/speaker_name"
      ],
      "metadata": {
        "id": "WwgRs06k1Z0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Don't overwrite existing file\n",
        "if not allow_overwrite_existing_corpus:\n",
        "  assert(not os.path.exists(corpus_file_path)), 'Corpus file already exists, enable `allow_overwrite_existing_corpus` to disable this behavior.'\n",
        "\n",
        "\n",
        "def concatenate_file_contents(filename):\n",
        "    \"\"\" Reads text file and outputs string with name of file and contents \"\"\"\n",
        "    filename_no_ext = str(os.path.basename(filename)).replace(text_file_extension,'')\n",
        "    with open(filename, 'r') as file:\n",
        "      contents = file.read().strip()\n",
        "      result = f\"{filename_no_ext}|{contents}|{contents}\\r\\n\"\n",
        "      return result\n",
        "\n",
        "\n",
        "def process_files_in_path(text_files_path, output_corpus_file_path):\n",
        "    \"\"\" Open a file at output_corpus_path and write formatted data to it \"\"\"\n",
        "    with open(output_corpus_file_path, 'w') as f:\n",
        "        # Get all .txt files in the specified path\n",
        "        txt_files = [file for file in os.listdir(text_files_path) if file.endswith(text_file_extension)]\n",
        "        txt_files_count = len(txt_files)\n",
        "        if txt_files_count <= 0:\n",
        "          print(f'No text files with extension {text_file_extension} found in {text_files_path}, try changing `text_file_extension` in settings')\n",
        "        # Process each file and concatenate the contents\n",
        "        for file in txt_files:\n",
        "          file_path = os.path.join(text_files_path, file)\n",
        "          output = concatenate_file_contents(file_path)\n",
        "          f.write(output)\n",
        "# Run\n",
        "print(f'Dataset path: {dataset_path}')\n",
        "print(f'Corpus path: {corpus_file_path}')\n",
        "process_files_in_path(dataset_path, corpus_file_path)\n",
        "print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT9s7mZH1OMa",
        "outputId": "c555130e-19ed-4d02-cd88-78f6fc8bf74f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset path: /gdrive/MyDrive/subset\n",
            "Corpus path: /content/output_dataset/raw_data/universal/metadata.csv\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make TextGrids"
      ],
      "metadata": {
        "id": "f_lDZDAzWYDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new configuration files"
      ],
      "metadata": {
        "id": "dG88YPpkWe2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "try:\n",
        "    from yaml import CLoader as Loader, CDumper as Dumper\n",
        "except ImportError:\n",
        "    from yaml import Loader, Dumper\n",
        "\n",
        "config_dir = f'/content/FastSpeech2/config/{dataset_name}'\n",
        "copied_config_dir = os.path.join(output_path, 'configs')\n",
        "\n",
        "!mkdir -p $config_dir \n",
        "!mkdir $copied_config_dir\n",
        "!cp -r /content/FastSpeech2/config/LJSpeech/* $config_dir\n",
        "\n",
        "\n",
        "def get_yaml_path(name):\n",
        "  return os.path.join(config_dir, name+'.yaml')\n",
        "\n",
        "\n",
        "def get_yaml_contents(name):\n",
        "  with open(get_yaml_path(name), 'r') as f:\n",
        "    return yaml.safe_load(f.read())\n",
        "  \n",
        "\n",
        "def write_yaml(name, contents):\n",
        "  with open(get_yaml_path(name), 'w') as f:\n",
        "      f.write(yaml.dump(contents))\n",
        "\n",
        "\n",
        "# model.yaml - change speaker name\n",
        "model = get_yaml_contents('model')\n",
        "model['vocoder']['speaker'] = speaker_name\n",
        "write_yaml('model', model)\n",
        "\n",
        "# preprocess.yaml - update paths and add field to text\n",
        "pp = get_yaml_contents('preprocess')\n",
        "pp['dataset'] = dataset_name\n",
        "pp['path']['corpus_path'] = corpus_path\n",
        "pp['path']['lexicon_path'] = lexicon_path\n",
        "pp['path']['raw_path'] = raw_data_path\n",
        "pp['path']['preprocessed_path'] = preprocessed_data_speaker_path\n",
        "pp['preprocessing']['text']['max_length'] = 4096  # Needed for training EfficientSpeech models\n",
        "write_yaml('preprocess', pp)\n",
        "\n",
        "# train.yaml - update paths\n",
        "tr = get_yaml_contents('train')\n",
        "tr['path']['ckpt_path'] = f'./output/ckpt/{dataset_name}'\n",
        "tr['path']['log_path'] = f'./output/log/{dataset_name}'\n",
        "tr['path']['result_path'] = f'./output/result/{dataset_name}'\n",
        "write_yaml('train', tr)\n",
        "\n",
        "print(f'Wrote configs in {config_dir}, copying to {copied_config_dir}')\n",
        "!cp -r $config_dir $copied_config_dir\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "aMx1ij0HWbLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28de5b5-4357-49c3-ef43-ffd6320422f7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/output_dataset/configs’: File exists\n",
            "Wrote configs in /content/FastSpeech2/config/MyDataset, copying to /content/output_dataset/configs\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare align\n",
        "The following code is modified from https://github.com/ming024/FastSpeech2/blob/master/preprocessor/ljspeech.py\n"
      ],
      "metadata": {
        "id": "xIxc6NUtfuKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "try:\n",
        "    from yaml import CLoader as Loader, CDumper as Dumper\n",
        "except ImportError:\n",
        "    from yaml import Loader, Dumper\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Workaround for importing text\n",
        "import sys\n",
        "sys.path.append('/content/FastSpeech2')\n",
        "from text import _clean_text\n",
        "\n",
        "\n",
        "def prepare_align(config):\n",
        "    sampling_rate = config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n",
        "    max_wav_value = config[\"preprocessing\"][\"audio\"][\"max_wav_value\"]\n",
        "    cleaners = config[\"preprocessing\"][\"text\"][\"text_cleaners\"]\n",
        "    speaker = speaker_name\n",
        "    with open(corpus_file_path, encoding=\"utf-8\") as f:\n",
        "        for line in tqdm(f):\n",
        "            parts = line.strip().split(\"|\")\n",
        "            base_name = parts[0]\n",
        "            text = parts[2]\n",
        "            text = _clean_text(text, cleaners)\n",
        "\n",
        "            wav_path = os.path.join(dataset_path, \"{}.wav\".format(base_name))\n",
        "            if os.path.exists(wav_path):\n",
        "                os.makedirs(raw_data_speaker_path, exist_ok=True)\n",
        "                wav, sr = librosa.load(wav_path, sr=sampling_rate)\n",
        "                #wav, _ = librosa.load(wav_path, sampling_rate)\n",
        "                wav = wav / max(abs(wav)) * max_wav_value\n",
        "                wavfile.write(\n",
        "                    os.path.join(raw_data_speaker_path, \"{}.wav\".format(base_name)),\n",
        "                    sampling_rate,\n",
        "                    wav.astype(np.int16),\n",
        "                )\n",
        "                with open(\n",
        "                    os.path.join(raw_data_speaker_path, \"{}.lab\".format(base_name)),\n",
        "                    \"w\",\n",
        "                ) as f1:\n",
        "                    f1.write(text)\n",
        "\n",
        "\n",
        "config = get_yaml_contents('preprocess')\n",
        "prepare_align(config)\n",
        "print('Prepare align done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PXl_dbkf6xk",
        "outputId": "de90169f-4696-4744-bc2c-edc260d3e7ad"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9it [00:02,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare align done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Forced alignment\n"
      ],
      "metadata": {
        "id": "kHqaO0dcCofH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1RxsTDzkEC__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output TextGrid files go here\n",
        "!su - mfa_user -c \"mkdir -p $mfa_output_path\"\n",
        "%mkdir -p $textgrid_dir\n",
        "\n",
        "# Allow mfa_user access to output directory \n",
        "!chown mfa_user $textgrid_dir\n",
        "\n",
        "# Command line options\n",
        "# -m fast: immediate disconnect (doesn't work sadface)\n",
        "# --clean: cleans output dir for subsequent runs (if off, \n",
        "#             does not overwrite old data)\n",
        "# --single_speaker: multiprocessing for only one speaker\n",
        "mfa_cmd_opts = f'--clean --single_speaker'\n",
        "align_cmd_opts = f'{corpus_path} {dictionary_file} {acoustic_model} {mfa_output_path}'\n",
        "\n",
        "# Command must be run as unprivileged user\n",
        "!echo Running mfa align with arguments: $mfa_cmd_opts $align_cmd_opts\n",
        "!su - mfa_user -c \"mfa align $mfa_cmd_opts $align_cmd_opts\"\n",
        "\n",
        "!echo Copying TextGrid files to $textgrid_dir\n",
        "!cp $mfa_output_path/*.* $textgrid_dir "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrR33GuhDArV",
        "outputId": "ceed8f30-359a-4b43-c58e-fb6302192608"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running mfa align with arguments: --clean --single_speaker /content/output_dataset/raw_data/universal /content/FastSpeech2/lexicon/librispeech-lexicon.txt english_us_arpa /home/mfa_user/MyDataset/TextGrid\n",
            "waiting for server to start.... done\n",
            "server started\n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n",
            "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m9\u001b[0m files, average number of utterances per      \n",
            "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m9.0\u001b[0m                                                          \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n",
            "\u001b[2K\u001b[35m  44%\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/9 \u001b[0m [ \u001b[33m0:00:24\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m16 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split for feature generation\u001b[33m...\u001b[0m                       \n",
            "\u001b[2K\u001b[35m  72%\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m13/18 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m86 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n",
            "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9 \u001b[0m [ \u001b[33m0:00:08\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m2 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n",
            "\u001b[2K\u001b[35m  44%\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/9 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split with features\u001b[33m...\u001b[0m                                \n",
            "\u001b[2K\u001b[35m  78%\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m7/9 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m56 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
            "\u001b[2K\u001b[35m  67%\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m6/9 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m26 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n",
            "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m30 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating fMLLR for speaker adaptation\u001b[33m...\u001b[0m                           \n",
            "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing second-pass alignment\u001b[33m...\u001b[0m                                   \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n",
            "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9 \u001b[0m [ \u001b[33m0:00:03\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m28 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n",
            "\u001b[2K\u001b[35m  78%\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m7/9 \u001b[0m [ \u001b[33m0:00:13\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m15 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n",
            "\u001b[2K\u001b[35m  78%\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m7/9 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m55 it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to \u001b[35m/home/mfa_user/MyDataset/\u001b[0m\u001b[95mTextGrid...\u001b[0m \n",
            "\u001b[2K\u001b[35m  11%\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/9 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[?25h\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to \u001b[35m/home/mfa_user/MyDataset/\u001b[0m\u001b[95mTextGrid\u001b[0m!    \n",
            "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m158.281\u001b[0m seconds                                 \n",
            "pg_ctl stdout: waiting for server to shut down............................................................... failed\n",
            "\n",
            "pg_ctl stderr: pg_ctl: server does not shut down\n",
            "HINT: The \"-m fast\" option immediately disconnects sessions rather than\n",
            "waiting for session-initiated disconnection.\n",
            "\n",
            "Exception ignored in atexit callback: <function stop_server at 0x7fa9cd8b9d80>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/site-packages/montreal_forced_aligner/command_line/utils.py\", line 470, in stop_server\n",
            "    raise DatabaseError(\n",
            "montreal_forced_aligner.exceptions.DatabaseError: DatabaseError:\n",
            "\n",
            "There was an error encountered starting the global MFA database server, please see /home/mfa_user/Documents/MFA/pg_log_global.txt for more details and/or look at the logged errors above.\n",
            "\u001b[0mCopying TextGrid files to /content/output_dataset/preprocessed_data/MyDataset/TextGrid/universal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess textgrids"
      ],
      "metadata": {
        "id": "sWT4zYzl47Vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you run into errors, install old version of librosa\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/FastSpeech2')\n",
        "from preprocessor.preprocessor import Preprocessor\n",
        "\n",
        "config = get_yaml_contents('preprocess')\n",
        "preprocessor = Preprocessor(config)\n",
        "preprocessor.build_from_path()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "eEjO6Np05TFh",
        "outputId": "3db89a59-2d2e-462d-df9b-5c73b49ecfe5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/FastSpeech2/audio/stft.py:42: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  fft_window = pad_center(fft_window, filter_length)\n",
            "/content/FastSpeech2/audio/stft.py:145: FutureWarning: Pass sr=22050, n_fft=1024, n_mels=80, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  mel_basis = librosa_mel_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Data ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-b1dd6742f25f>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_yaml_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preprocess'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/FastSpeech2/preprocessor/preprocessor.py\u001b[0m in \u001b[0;36mbuild_from_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 )\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeaker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/FastSpeech2/preprocessor/preprocessor.py\u001b[0m in \u001b[0;36mprocess_utterance\u001b[0;34m(self, speaker, basename)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# Compute mel-scale spectrogram and energy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mel_from_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTFT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0mmel_spectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/FastSpeech2/audio/tools.py\u001b[0m in \u001b[0;36mget_mel_from_wav\u001b[0;34m(audio, _stft)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmelspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmelspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmelspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/FastSpeech2/audio/stft.py\u001b[0m in \u001b[0;36mmel_spectrogram\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mmagnitudes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstft_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mmagnitudes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagnitudes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mmel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmel_basis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnitudes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/FastSpeech2/audio/stft.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         forward_transform = F.conv1d(\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_basis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhop_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    }
  ]
}